{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "23FSvPla9GDC"
      },
      "outputs": [],
      "source": [
        "# import python file from parent folder\n",
        "from img_cap_lib import *\n",
        "# imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torchtext\n",
        "from torchtext.vocab import vocab, GloVe, Vectors\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "from PIL import Image\n",
        "import string\n",
        "from collections import OrderedDict, Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import os\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJIT25pzc_h5",
        "outputId": "715891ec-bf99-4f0b-f030-a031c6ca8542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not using google colab\n"
          ]
        }
      ],
      "source": [
        "# check if google colab is installed\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except:\n",
        "    print(\"Not using google colab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modellpfad definieren"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = \"tiny_model_with_normalisation.pt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImcPlOrB9GDG"
      },
      "source": [
        "# Daten herunterladen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or3FdVAM9GDH",
        "outputId": "853fc1fc-846b-4295-9343-4a2edfa65ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data already exi sts at flickr8k_tiny\n"
          ]
        }
      ],
      "source": [
        "data_download(\"flickr8k_tiny\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZovGtMKV9GDK"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdgY3n_J9GDK",
        "outputId": "9d4676f3-f9ab-4e98-cdb4-683a9d3d42d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape captions: (40460, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ronnyschneeberger/Documents/FHNW/HS22/del-image-captioning/img_cap_lib.py:108: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.captions.caption = self.captions.caption.apply(lambda x: x.strip(\".\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape captions after filtering: (39749, 3)\n",
            "Removed Captions:  711 , in Percent:  1.76\n",
            "transformed_images folder already exists. No preprocessing necessary.\n"
          ]
        }
      ],
      "source": [
        "# caption preprocessing\n",
        "embedding_dim = 300\n",
        "min_frequency = 1\n",
        "\n",
        "captions = pd.read_csv(\"flickr8k_tiny/captions.txt\")\n",
        "caption_preprocessor = CaptionPreprocessor(captions=captions, captions_path=\"flickr8k_tiny/captions.txt\", embedding_dim=embedding_dim, min_frequency=min_frequency)\n",
        "caption_preprocessor.preprocess()\n",
        "\n",
        "# image preprocessing\n",
        "img_preprocessor = ImagePreprocessor(image_size=(112,112), normalize=True, image_folder_path=\"flickr8k_tiny\")\n",
        "img_preprocessor.preprocess_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nl3NT_K9GDL"
      },
      "source": [
        "# Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "aQ07VI9R9GDM"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "training_data, test_data = train_test_split(caption_preprocessor.captions, test_size=0.15, random_state=42)\n",
        "\n",
        "embedding = Embedding(embedding_matrix=caption_preprocessor.embedding, vocabulary=caption_preprocessor.vocabulary)\n",
        "\n",
        "# create dataset\n",
        "train_dataset = FlickrDataset(captions=training_data, embedding=embedding, image_folder=\"flickr8k_tiny\")\n",
        "test_dataset = FlickrDataset(captions=test_data, embedding=embedding, image_folder=\"flickr8k_tiny\")\n",
        "\n",
        "# create dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-254t2-t9GDN"
      },
      "source": [
        "# Modell erstellen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [18], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model \u001b[39m=\u001b[39m load_captioning_model(model_stats\u001b[39m=\u001b[39mmodel_stats)\n\u001b[1;32m     10\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model_stats \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrain_model(loader\u001b[39m=\u001b[39mtrain_loader, optimizer\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m), criterion\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss(), epochs\u001b[39m=\u001b[39m\u001b[39m250\u001b[39m, print_every\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39m# save model\u001b[39;00m\n\u001b[1;32m     14\u001b[0m torch\u001b[39m.\u001b[39msave(model_stats, model_path)\n",
            "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/img_cap_lib.py:615\u001b[0m, in \u001b[0;36mImageCaptioning.train_model\u001b[0;34m(self, loader, optimizer, criterion, epochs, print_every)\u001b[0m\n\u001b[1;32m    612\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m    614\u001b[0m \u001b[39m# backpropagate loss\u001b[39;00m\n\u001b[0;32m--> 615\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    617\u001b[0m \u001b[39m# update weights\u001b[39;00m\n\u001b[1;32m    618\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
            "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
            "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# create model\n",
        "# encoder = EncoderCNN(net=torchvision.models.convnext_tiny, pretrained_weights=torchvision.models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1, output_size=300)\n",
        "# decoder = DecoderRNN(input_size=300, hidden_size=caption_preprocessor.embedding_dim, num_layers=1, dropout=0.0, len_vocab=len(embedding.words))\n",
        "# embedding = Embedding(embedding_matrix=caption_preprocessor.embedding, vocabulary=caption_preprocessor.vocabulary)\n",
        "# model = ImageCaptioning(encoder=encoder, decoder=decoder, embedding=embedding, batch_size=batch_size)\n",
        "\n",
        "model_stats = torch.load(\"models/tiny_model_with_normalisation.pt\", map_location=torch.device(device))\n",
        "model = load_captioning_model(model_stats=model_stats)\n",
        "\n",
        "# train model\n",
        "model_stats = model.train_model(loader=train_loader, optimizer=torch.optim.Adam(model.parameters(), lr=0.001), criterion=torch.nn.CrossEntropyLoss(), epochs=1, print_every=20)\n",
        "\n",
        "# save model\n",
        "torch.save(model_stats, model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Bncn1u9GDP"
      },
      "source": [
        "# Evaluierung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HcRCpW29GDQ"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, model, dataloader, device):\n",
        "        # initiate variables \n",
        "        self.model = model\n",
        "        self.dataloader = dataloader\n",
        "        self.device = device\n",
        "    \n",
        "    def evaluate(self):\n",
        "        scores = []\n",
        "\n",
        "        for i, (images, captions, lengths, vectorized_captions) in enumerate(self.dataloader):\n",
        "            # move to device\n",
        "            images = images.to(self.device)\n",
        "            captions = captions.to(self.device)\n",
        "            vectorized_captions = vectorized_captions.to(self.device)\n",
        "            \n",
        "            # forward pass\n",
        "            output = self.model.forward(images)\n",
        "            references = self.model.words[vectorized_captions.cpu()]\n",
        "\n",
        "            for j in range(output.shape[0]):\n",
        "                candidate = self.output_to_sentence(output[j,:])\n",
        "                reference = self.output_to_sentence(references[j,:])\n",
        "                scores.append(self.bleu_score(candidate, reference))\n",
        "            \n",
        "            print(f\"Batch: {i+1} of {len(self.dataloader)}\")\n",
        "\n",
        "        print(f\"Average BLEU score: {np.mean(scores)}\")\n",
        "        return np.mean(scores), scores\n",
        "\n",
        "    @staticmethod\n",
        "    def output_to_sentence(output:list):\n",
        "        '''\n",
        "        Removes Tokens from model output.\n",
        "        '''\n",
        "        output = [token for token in output if token not in [\"<SOS>\", \"<EOS>\", \"<PAD>\"]]\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def bleu_score(reference, candidate):\n",
        "        '''\n",
        "        Calculates the BLEU score for a single reference and candidate. Uses the SmoothingFunction for smoothing when no overlap between certain n-grams is found. \n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "        reference: list of strings - The reference sentence.\n",
        "        candidate: list of strings - The candidate sentence.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        bleu_score: float - The BLEU score.\n",
        "        '''\n",
        "        # calculate the BLEU score\n",
        "        return nltk.translate.bleu_score.sentence_bleu(reference, candidate, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NttpE3AhHt1M"
      },
      "outputs": [],
      "source": [
        "path = model_path\n",
        "model_stats = torch.load(path, map_location=device)\n",
        "model = load_captioning_model(model_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U331derr9GDR"
      },
      "outputs": [],
      "source": [
        "train_dataset = FlickrDataset(captions=training_data, embedding=embedding, image_folder=\"flickr8k_tiny\")\n",
        "test_dataset = FlickrDataset(captions=test_data, embedding=embedding, image_folder=\"flickr8k_tiny\")\n",
        "\n",
        "# create dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "# calculate bleu scores\n",
        "train_evaluator = Evaluator(model, train_loader, device)\n",
        "test_evaluator = Evaluator(model, test_loader, device)\n",
        "\n",
        "train_bleu, train_scores = train_evaluator.evaluate()\n",
        "test_bleu, test_scores = test_evaluator.evaluate()\n",
        "\n",
        "print(f\"Train BLEU: {train_bleu}\")\n",
        "print(f\"Test BLEU: {test_bleu}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7ggCETV9GDS"
      },
      "outputs": [],
      "source": [
        "# export bleu scores\n",
        "with open(\"tiny_train_scores.pkl\", \"wb\") as f:\n",
        "    pickle.dump(train_scores, f)\n",
        "\n",
        "with open(\"tiny_test_scores.pkl\", \"wb\") as f:\n",
        "    pickle.dump(test_scores, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.15 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "8c062c6b57d91616a29c64ccda85a992f94b9c302ff6b9e6bdfcfbfa090602a1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
