{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "23FSvPla9GDC"
      },
      "outputs": [],
      "source": [
        "# import python file from parent folder\n",
        "from img_cap_lib import *\n",
        "# imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torchtext\n",
        "from torchtext.vocab import vocab, GloVe, Vectors\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "from PIL import Image\n",
        "import string\n",
        "from collections import OrderedDict, Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import os\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJIT25pzc_h5",
        "outputId": "715891ec-bf99-4f0b-f030-a031c6ca8542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not using google colab\n"
          ]
        }
      ],
      "source": [
        "# check if google colab is installed\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "except:\n",
        "    print(\"Not using google colab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modellpfad definieren"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = \"standart_model_with_normalisation.pt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImcPlOrB9GDG"
      },
      "source": [
        "# Daten herunterladen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or3FdVAM9GDH",
        "outputId": "853fc1fc-846b-4295-9343-4a2edfa65ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data already exi sts at flickr8k\n"
          ]
        }
      ],
      "source": [
        "data_download(\"flickr8k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZovGtMKV9GDK"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdgY3n_J9GDK",
        "outputId": "9d4676f3-f9ab-4e98-cdb4-683a9d3d42d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape captions: (40460, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ronnyschneeberger/Documents/FHNW/HS22/del-image-captioning/img_cap_lib.py:108: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.captions.caption = self.captions.caption.apply(lambda x: x.strip(\".\"))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape captions after filtering: (39749, 3)\n",
            "Removed Captions:  711 , in Percent:  1.76\n",
            "transformed_images folder already exists. No preprocessing necessary.\n"
          ]
        }
      ],
      "source": [
        "# caption preprocessing\n",
        "embedding_dim = 300\n",
        "min_frequency = 1\n",
        "\n",
        "captions = pd.read_csv(\"flickr8k/captions.txt\")\n",
        "caption_preprocessor = CaptionPreprocessor(captions=captions, captions_path=\"flickr8k/captions.txt\", embedding_dim=embedding_dim, min_frequency=min_frequency)\n",
        "caption_preprocessor.preprocess()\n",
        "\n",
        "# image preprocessing\n",
        "img_preprocessor = ImagePreprocessor(normalize=True, image_folder_path=\"flickr8k\")\n",
        "img_preprocessor.preprocess_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nl3NT_K9GDL"
      },
      "source": [
        "# Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aQ07VI9R9GDM"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "training_data, test_data = train_test_split(caption_preprocessor.captions, test_size=0.15, random_state=42)\n",
        "\n",
        "embedding = Embedding(embedding_matrix=caption_preprocessor.embedding, vocabulary=caption_preprocessor.vocabulary)\n",
        "\n",
        "# create dataset\n",
        "train_dataset = FlickrDataset(captions=training_data, embedding=embedding, image_folder=\"flickr8k\")\n",
        "test_dataset = FlickrDataset(captions=test_data, embedding=embedding, image_folder=\"flickr8k\")\n",
        "\n",
        "# create dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-254t2-t9GDN"
      },
      "source": [
        "# Modell erstellen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create model\n",
        "encoder = EncoderCNN(net=torchvision.models.resnext50_32x4d, pretrained_weights=torchvision.models.ResNeXt50_32X4D_Weights.IMAGENET1K_V2, output_size=300)\n",
        "decoder = DecoderRNN(input_size=300, hidden_size=caption_preprocessor.embedding_dim, num_layers=1, dropout=0.0, len_vocab=len(embedding.words))\n",
        "embedding = Embedding(embedding_matrix=caption_preprocessor.embedding, vocabulary=caption_preprocessor.vocabulary)\n",
        "model = ImageCaptioning(encoder=encoder, decoder=decoder, embedding=embedding, batch_size=batch_size)\n",
        "\n",
        "# train model\n",
        "model_stats = model.train_model(loader=train_loader, optimizer=torch.optim.Adam(model.parameters(), lr=0.001), criterion=torch.nn.CrossEntropyLoss(), epochs=250, print_every=20)\n",
        "\n",
        "# save model\n",
        "torch.save(model_stats, model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Bncn1u9GDP"
      },
      "source": [
        "# Evaluierung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HcRCpW29GDQ"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, model, dataloader, device):\n",
        "        # initiate variables \n",
        "        self.model = model\n",
        "        self.dataloader = dataloader\n",
        "        self.device = device\n",
        "    \n",
        "    def evaluate(self):\n",
        "        scores = []\n",
        "\n",
        "        for i, (images, captions, lengths, vectorized_captions) in enumerate(self.dataloader):\n",
        "            # move to device\n",
        "            images = images.to(self.device)\n",
        "            captions = captions.to(self.device)\n",
        "            vectorized_captions = vectorized_captions.to(self.device)\n",
        "            \n",
        "            # forward pass\n",
        "            output = self.model.forward(images)\n",
        "            references = self.model.words[vectorized_captions.cpu()]\n",
        "\n",
        "            for j in range(output.shape[0]):\n",
        "                candidate = self.output_to_sentence(output[j,:])\n",
        "                reference = self.output_to_sentence(references[j,:])\n",
        "                scores.append(self.bleu_score(candidate, reference))\n",
        "            \n",
        "            print(f\"Batch: {i+1} of {len(self.dataloader)}\")\n",
        "\n",
        "        print(f\"Average BLEU score: {np.mean(scores)}\")\n",
        "        return np.mean(scores), scores\n",
        "\n",
        "    @staticmethod\n",
        "    def output_to_sentence(output:list):\n",
        "        '''\n",
        "        Removes Tokens from model output.\n",
        "        '''\n",
        "        output = [token for token in output if token not in [\"<SOS>\", \"<EOS>\", \"<PAD>\"]]\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def bleu_score(reference, candidate):\n",
        "        '''\n",
        "        Calculates the BLEU score for a single reference and candidate. Uses the SmoothingFunction for smoothing when no overlap between certain n-grams is found. \n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "        reference: list of strings - The reference sentence.\n",
        "        candidate: list of strings - The candidate sentence.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        bleu_score: float - The BLEU score.\n",
        "        '''\n",
        "        # calculate the BLEU score\n",
        "        return nltk.translate.bleu_score.sentence_bleu(reference, candidate, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NttpE3AhHt1M"
      },
      "outputs": [],
      "source": [
        "path = model_path\n",
        "model_stats = torch.load(path, map_location=device)\n",
        "model = load_captioning_model(model_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U331derr9GDR"
      },
      "outputs": [],
      "source": [
        "train_dataset = FlickrDataset(captions=training_data, embedding=embedding, image_folder=\"flickr8k\")\n",
        "test_dataset = FlickrDataset(captions=test_data, embedding=embedding, image_folder=\"flickr8k\")\n",
        "\n",
        "# create dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "# calculate bleu scores\n",
        "train_evaluator = Evaluator(model, train_loader, device)\n",
        "test_evaluator = Evaluator(model, test_loader, device)\n",
        "\n",
        "train_bleu, train_scores = train_evaluator.evaluate()\n",
        "test_bleu, test_scores = test_evaluator.evaluate()\n",
        "\n",
        "print(f\"Train BLEU: {train_bleu}\")\n",
        "print(f\"Test BLEU: {test_bleu}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7ggCETV9GDS"
      },
      "outputs": [],
      "source": [
        "# export bleu scores\n",
        "with open(\"standart_train_scores.pkl\", \"wb\") as f:\n",
        "    pickle.dump(train_scores, f)\n",
        "\n",
        "with open(\"standart_train_scores.pkl\", \"wb\") as f:\n",
        "    pickle.dump(test_scores, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.15 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "8c062c6b57d91616a29c64ccda85a992f94b9c302ff6b9e6bdfcfbfa090602a1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
