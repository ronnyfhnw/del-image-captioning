{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ronnyschneeberger/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from img_cap_lib import *\n",
    "# imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchtext\n",
    "from torchtext.vocab import vocab, GloVe, Vectors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "from PIL import Image\n",
    "import string\n",
    "from collections import OrderedDict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exi sts at flickr8k\n"
     ]
    }
   ],
   "source": [
    "data_download(\"flickr8k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_stats = torch.load(\"models/without_normalisation.pt\", map_location=torch.device('cpu'))\n",
    "model = load_captioning_model(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape captions: (40460, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ronnyschneeberger/Documents/FHNW/HS22/del-image-captioning/img_cap_lib.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.captions.caption = self.captions.caption.apply(lambda x: x.strip(\".\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape captions after filtering: (39749, 3)\n",
      "Removed Captions:  711 , in Percent:  1.76\n",
      "transformed_images folder already exists. No preprocessing necessary.\n"
     ]
    }
   ],
   "source": [
    "# caption preprocessing\n",
    "embedding_dim = 300\n",
    "min_frequency = 1\n",
    "\n",
    "captions = pd.read_csv(\"flickr8k/captions.txt\")\n",
    "caption_preprocessor = CaptionPreprocessor(embedding=model_stats['embedding'].embedding_matrix, vocabulary=model_stats['embedding'].vocabulary ,captions=captions, embedding_dim=embedding_dim, min_frequency=min_frequency)\n",
    "caption_preprocessor.preprocess()\n",
    "\n",
    "# image preprocessing\n",
    "img_preprocessor = ImagePreprocessor(normalize=False, image_folder_path=\"flickr8k\")\n",
    "img_preprocessor.preprocess_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datensplit und DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create split\n",
    "training_data, test_data = train_test_split(caption_preprocessor.captions, test_size=0.15, random_state=42)\n",
    "\n",
    "# create datasets\n",
    "train_dataset = FlickrDataset(captions=training_data, embedding=model.embedding)\n",
    "test_dataset = FlickrDataset(captions=test_data, embedding=model.embedding)\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = 4\n",
    "train_loader = FlickrLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = FlickrLoader(test_dataset, batch_size=4, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>caption_word_length</th>\n",
       "      <th>vectorized_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34401</th>\n",
       "      <td>368954110_821ccf005c.jpg</td>\n",
       "      <td>['&lt;SOS&gt;', 'three', 'children', 'sit', 'on', 't...</td>\n",
       "      <td>6</td>\n",
       "      <td>[3, 48, 60, 167, 7, 6, 42, 4, 1, 1, 1, 1, 1, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13250</th>\n",
       "      <td>2687529141_edee32649e.jpg</td>\n",
       "      <td>['&lt;SOS&gt;', 'acrobatic', 'entertainers', 'with',...</td>\n",
       "      <td>8</td>\n",
       "      <td>[3, 2713, 6287, 11, 456, 378, 887, 71, 652, 4,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29817</th>\n",
       "      <td>3501206996_477be0f318.jpg</td>\n",
       "      <td>['&lt;SOS&gt;', 'a', 'child', 'kicks', 'a', 'soccer'...</td>\n",
       "      <td>6</td>\n",
       "      <td>[3, 2, 43, 585, 2, 106, 39, 4, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>108898978_7713be88fc.jpg</td>\n",
       "      <td>['&lt;SOS&gt;', 'skiiers', 'walking', 'up', 'the', '...</td>\n",
       "      <td>8</td>\n",
       "      <td>[3, 1928, 61, 55, 6, 134, 34, 2, 292, 4, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36999</th>\n",
       "      <td>470373679_98dceb19e7.jpg</td>\n",
       "      <td>['&lt;SOS&gt;', 'sillhouttes', 'of', 'people', 'in',...</td>\n",
       "      <td>14</td>\n",
       "      <td>[3, 8373, 13, 24, 5, 50, 13, 2, 107, 9, 5, 50,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6391</th>\n",
       "      <td>2231847779_1148d1c919.jpg</td>\n",
       "      <td>['&lt;SOS&gt;', 'a', 'baby', 'with', 'food', 'smeare...</td>\n",
       "      <td>14</td>\n",
       "      <td>[3, 2, 144, 11, 447, 3584, 7, 74, 124, 9, 14, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11510</th>\n",
       "      <td>2568417021_afa68423e5.jpg</td>\n",
       "      <td>['&lt;SOS&gt;', 'a', 'boy', 'gets', 'ready', 'to', '...</td>\n",
       "      <td>8</td>\n",
       "      <td>[3, 2, 17, 391, 353, 21, 1242, 2, 150, 4, 1, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38842</th>\n",
       "      <td>576093768_e78f91c176.jpg</td>\n",
       "      <td>['&lt;SOS&gt;', 'several', 'young', 'boys', 'looking...</td>\n",
       "      <td>8</td>\n",
       "      <td>[3, 182, 26, 96, 89, 65, 2, 984, 2904, 4, 1, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>1211015912_9f3ee3a995.jpg</td>\n",
       "      <td>['&lt;SOS&gt;', 'children', 'at', 'a', 'park', '&lt;EOS...</td>\n",
       "      <td>4</td>\n",
       "      <td>[3, 60, 23, 2, 119, 4, 1, 1, 1, 1, 1, 1, 1, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16098</th>\n",
       "      <td>2876993733_cb26107d18.jpg</td>\n",
       "      <td>['&lt;SOS&gt;', 'two', 'people', 'in', 'green', 'sit...</td>\n",
       "      <td>10</td>\n",
       "      <td>[3, 14, 24, 5, 58, 167, 5, 29, 3819, 709, 810,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33782 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "34401   368954110_821ccf005c.jpg   \n",
       "13250  2687529141_edee32649e.jpg   \n",
       "29817  3501206996_477be0f318.jpg   \n",
       "350     108898978_7713be88fc.jpg   \n",
       "36999   470373679_98dceb19e7.jpg   \n",
       "...                          ...   \n",
       "6391   2231847779_1148d1c919.jpg   \n",
       "11510  2568417021_afa68423e5.jpg   \n",
       "38842   576093768_e78f91c176.jpg   \n",
       "879    1211015912_9f3ee3a995.jpg   \n",
       "16098  2876993733_cb26107d18.jpg   \n",
       "\n",
       "                                                 caption  caption_word_length  \\\n",
       "34401  ['<SOS>', 'three', 'children', 'sit', 'on', 't...                    6   \n",
       "13250  ['<SOS>', 'acrobatic', 'entertainers', 'with',...                    8   \n",
       "29817  ['<SOS>', 'a', 'child', 'kicks', 'a', 'soccer'...                    6   \n",
       "350    ['<SOS>', 'skiiers', 'walking', 'up', 'the', '...                    8   \n",
       "36999  ['<SOS>', 'sillhouttes', 'of', 'people', 'in',...                   14   \n",
       "...                                                  ...                  ...   \n",
       "6391   ['<SOS>', 'a', 'baby', 'with', 'food', 'smeare...                   14   \n",
       "11510  ['<SOS>', 'a', 'boy', 'gets', 'ready', 'to', '...                    8   \n",
       "38842  ['<SOS>', 'several', 'young', 'boys', 'looking...                    8   \n",
       "879    ['<SOS>', 'children', 'at', 'a', 'park', '<EOS...                    4   \n",
       "16098  ['<SOS>', 'two', 'people', 'in', 'green', 'sit...                   10   \n",
       "\n",
       "                                      vectorized_caption  \n",
       "34401  [3, 48, 60, 167, 7, 6, 42, 4, 1, 1, 1, 1, 1, 1...  \n",
       "13250  [3, 2713, 6287, 11, 456, 378, 887, 71, 652, 4,...  \n",
       "29817  [3, 2, 43, 585, 2, 106, 39, 4, 1, 1, 1, 1, 1, ...  \n",
       "350    [3, 1928, 61, 55, 6, 134, 34, 2, 292, 4, 1, 1,...  \n",
       "36999  [3, 8373, 13, 24, 5, 50, 13, 2, 107, 9, 5, 50,...  \n",
       "...                                                  ...  \n",
       "6391   [3, 2, 144, 11, 447, 3584, 7, 74, 124, 9, 14, ...  \n",
       "11510  [3, 2, 17, 391, 353, 21, 1242, 2, 150, 4, 1, 1...  \n",
       "38842  [3, 182, 26, 96, 89, 65, 2, 984, 2904, 4, 1, 1...  \n",
       "879    [3, 60, 23, 2, 119, 4, 1, 1, 1, 1, 1, 1, 1, 1,...  \n",
       "16098  [3, 14, 24, 5, 58, 167, 5, 29, 3819, 709, 810,...  \n",
       "\n",
       "[33782 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model, dataloader, device):\n",
    "        # initiate variables \n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        # self.model.eval()\n",
    "        # assert self.dataloader.batch_size == 1, \"Batch size must be 1 for evaluation.\"\n",
    "    \n",
    "    def evaluate(self):\n",
    "        scores = []\n",
    "\n",
    "        for i, (images, captions, lengths, vectorized_captions) in enumerate(self.dataloader):\n",
    "            # move to device\n",
    "            images = images.to(self.device)\n",
    "            captions = captions.to(self.device)\n",
    "            vectorized_captions = vectorized_captions.to(self.device)\n",
    "            \n",
    "            # forward pass\n",
    "            output = self.model.forward(images)\n",
    "            references = self.model.words[vectorized_captions.cpu()]\n",
    "\n",
    "            for j in range(output.shape[0]):\n",
    "                candidate = self.output_to_sentence(output[j,:])\n",
    "                reference = self.output_to_sentence(references[j,:])\n",
    "                scores.append(self.bleu_score(candidate, reference))\n",
    "            \n",
    "            print(f\"Batch: {i+1} of {len(self.dataloader)}\")\n",
    "\n",
    "        print(f\"Average BLEU score: {np.mean(scores)}\")\n",
    "        return np.mean(scores), scores\n",
    "\n",
    "    @staticmethod\n",
    "    def output_to_sentence(output:list):\n",
    "        '''\n",
    "        Removes Tokens from model output.\n",
    "        '''\n",
    "        output = [token for token in output if token not in [\"<SOS>\", \"<EOS>\", \"<PAD>\"]]\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def bleu_score(reference, candidate):\n",
    "        '''\n",
    "        Calculates the BLEU score for a single reference and candidate. Uses the SmoothingFunction for smoothing when no overlap between certain n-grams is found. \n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        reference: list of strings - The reference sentence.\n",
    "        candidate: list of strings - The candidate sentence.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        bleu_score: float - The BLEU score.\n",
    "        '''\n",
    "        # calculate the BLEU score\n",
    "        return nltk.translate.bleu_score.sentence_bleu(reference, candidate, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [60], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m test_evaluator \u001b[39m=\u001b[39m Evaluator(model, test_loader, device)\n\u001b[1;32m      4\u001b[0m \u001b[39m# train_bleu, train_scores = train_evaluator.evaluate()\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m test_bleu, test_scores \u001b[39m=\u001b[39m test_evaluator\u001b[39m.\u001b[39mevaluate()\n\u001b[1;32m      7\u001b[0m \u001b[39m# print(f\"Train BLEU: {train_bleu}\")\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest BLEU: \u001b[39m\u001b[39m{\u001b[39;00mtest_bleu\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [56], line 20\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m vectorized_captions \u001b[39m=\u001b[39m vectorized_captions\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m \u001b[39m# forward pass\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mforward(images)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m candidate \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_to_sentence(output)\n\u001b[1;32m     22\u001b[0m reference \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_to_sentence(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39membedding\u001b[39m.\u001b[39mindex_to_caption(vectorized_captions\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m,\u001b[39m0\u001b[39m))[\u001b[39m0\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/img_cap_lib.py:498\u001b[0m, in \u001b[0;36mImageCaptioning.forward\u001b[0;34m(self, images, max_length)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[39mThis function is used for inference time. It takes a batch of images and returns a np.array containing the predicted captions. \u001b[39;00m\n\u001b[1;32m    488\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39m    captions list: A list with the length of batch_size containing the predicted captions.\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[39m# extract features from images\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder\u001b[39m.\u001b[39;49mforward(images)\n\u001b[1;32m    500\u001b[0m \u001b[39m# create tensor for storing indexes\u001b[39;00m\n\u001b[1;32m    501\u001b[0m indexes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/img_cap_lib.py:346\u001b[0m, in \u001b[0;36mEncoderCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m    341\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[39m    Params:\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[39m    -------\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[39m        x torch.Tensor: The input tensor that is used for feature extraction. Shape: (batch_size, channels, height, width)\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torchvision/models/resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    270\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m    271\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[0;32m--> 273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m    275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torchvision/models/resnet.py:158\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    155\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(out)\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     identity \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdownsample(x)\n\u001b[1;32m    160\u001b[0m out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m identity\n\u001b[1;32m    161\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    169\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    170\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    173\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    177\u001b[0m     bn_training,\n\u001b[1;32m    178\u001b[0m     exponential_average_factor,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    180\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/FHNW/HS22/del-image-captioning/venv/lib/python3.9/site-packages/torch/nn/functional.py:2438\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2436\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2438\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2439\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2440\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_evaluator = Evaluator(model, train_loader, device)\n",
    "test_evaluator = Evaluator(model, test_loader, device)\n",
    "\n",
    "# train_bleu, train_scores = train_evaluator.evaluate()\n",
    "test_bleu, test_scores = test_evaluator.evaluate()\n",
    "\n",
    "# print(f\"Train BLEU: {train_bleu}\")\n",
    "print(f\"Test BLEU: {test_bleu}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c062c6b57d91616a29c64ccda85a992f94b9c302ff6b9e6bdfcfbfa090602a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
