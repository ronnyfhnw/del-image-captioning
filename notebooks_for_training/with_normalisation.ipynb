{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23FSvPla9GDC"
      },
      "outputs": [],
      "source": [
        "# import python file from parent folder\n",
        "from img_cap_lib import *\n",
        "# imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torchtext\n",
        "from torchtext.vocab import vocab, GloVe, Vectors\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "from PIL import Image\n",
        "import string\n",
        "from collections import OrderedDict, Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import os\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFpN7FJu9Kd8",
        "outputId": "2ec16231-7d1f-4f76-be0f-8be69954205e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImcPlOrB9GDG"
      },
      "source": [
        "# Daten herunterladen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or3FdVAM9GDH",
        "outputId": "61f368ca-5c40-4485-cb1d-95e76cb067d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data already exi sts at flickr8k\n"
          ]
        }
      ],
      "source": [
        "data_download(\"flickr8k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZovGtMKV9GDK"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdgY3n_J9GDK",
        "outputId": "5c08f080-11ce-4de9-e16a-60a74567d9cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape captions: (40460, 2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape captions after filtering: (39749, 3)\n",
            "Removed Captions:  711 , in Percent:  1.76\n",
            "transformed_images folder already exists. No preprocessing necessary.\n"
          ]
        }
      ],
      "source": [
        "# caption preprocessing\n",
        "embedding_dim = 300\n",
        "min_frequency = 1\n",
        "\n",
        "captions = pd.read_csv(\"flickr8k/captions.txt\")\n",
        "caption_preprocessor = CaptionPreprocessor(captions=captions, embedding_dim=embedding_dim, min_frequency=min_frequency)\n",
        "caption_preprocessor.preprocess()\n",
        "\n",
        "# image preprocessing\n",
        "img_preprocessor = ImagePreprocessor(normalize=True, image_folder_path=\"flickr8k\")\n",
        "img_preprocessor.preprocess_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nl3NT_K9GDL"
      },
      "source": [
        "# Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ07VI9R9GDM"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "training_data, test_data = train_test_split(caption_preprocessor.captions, test_size=0.15, random_state=42)\n",
        "\n",
        "embedding = Embedding(embedding_matrix=caption_preprocessor.embedding, vocabulary=caption_preprocessor.vocabulary)\n",
        "\n",
        "# create dataset\n",
        "train_dataset = FlickrDataset(captions=training_data, embedding=embedding)\n",
        "test_dataset = FlickrDataset(captions=test_data, embedding=embedding)\n",
        "\n",
        "# create dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-254t2-t9GDN"
      },
      "source": [
        "# Modell erstellen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8MBnXU09GDN"
      },
      "outputs": [],
      "source": [
        "encoder = EncoderCNN(net=torchvision.models.resnext50_32x4d, pretrained_weights=torchvision.models.ResNeXt50_32X4D_Weights.IMAGENET1K_V2, output_size=300)\n",
        "decoder = DecoderRNN(input_size=300, hidden_size=caption_preprocessor.embedding_dim, num_layers=1, dropout=0.0, len_vocab=embedding.embedding_matrix.shape[0])\n",
        "\n",
        "model = ImageCaptioning(encoder=encoder, decoder=decoder, embedding=embedding, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02luogN-9GDO",
        "outputId": "59f326db-fff1-408b-b9bf-458e30175737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/250 | Batch: 1/527 | Loss: 8.97307014465332\n",
            "Epoch: 1/250 | Average Epoch Loss: 7.0130262908718395\n",
            "Epoch: 2/250 | Batch: 1/527 | Loss: 6.673590183258057\n",
            "Epoch: 2/250 | Average Epoch Loss: 6.482943236262342\n",
            "Epoch: 3/250 | Batch: 1/527 | Loss: 6.243509292602539\n",
            "Epoch: 3/250 | Average Epoch Loss: 6.282781223644568\n",
            "Epoch: 4/250 | Batch: 1/527 | Loss: 6.2735137939453125\n",
            "Epoch: 4/250 | Average Epoch Loss: 6.127988313135657\n",
            "Epoch: 5/250 | Batch: 1/527 | Loss: 6.0735249519348145\n",
            "Epoch: 5/250 | Average Epoch Loss: 6.000287556331569\n",
            "Epoch: 6/250 | Batch: 1/527 | Loss: 5.871740818023682\n",
            "Epoch: 6/250 | Average Epoch Loss: 5.891066057179866\n",
            "Epoch: 7/250 | Batch: 1/527 | Loss: 5.835323810577393\n",
            "Epoch: 7/250 | Average Epoch Loss: 5.7913342436079285\n",
            "Epoch: 8/250 | Batch: 1/527 | Loss: 5.660100936889648\n",
            "Epoch: 8/250 | Average Epoch Loss: 5.701445124407194\n",
            "Epoch: 9/250 | Batch: 1/527 | Loss: 5.768121719360352\n",
            "Epoch: 9/250 | Average Epoch Loss: 5.617565630056826\n",
            "Epoch: 10/250 | Batch: 1/527 | Loss: 5.548180103302002\n",
            "Epoch: 10/250 | Average Epoch Loss: 5.538798083390412\n",
            "Epoch: 11/250 | Batch: 1/527 | Loss: 5.637386798858643\n",
            "Epoch: 11/250 | Average Epoch Loss: 5.464975581449621\n",
            "Epoch: 12/250 | Batch: 1/527 | Loss: 5.325587272644043\n",
            "Epoch: 12/250 | Average Epoch Loss: 5.395338148953114\n",
            "Epoch: 13/250 | Batch: 1/527 | Loss: 5.452564716339111\n",
            "Epoch: 13/250 | Average Epoch Loss: 5.328219833591179\n",
            "Epoch: 14/250 | Batch: 1/527 | Loss: 5.342388153076172\n",
            "Epoch: 14/250 | Average Epoch Loss: 5.2642607752240815\n",
            "Epoch: 15/250 | Batch: 1/527 | Loss: 5.371570110321045\n",
            "Epoch: 15/250 | Average Epoch Loss: 5.202679120159692\n",
            "Epoch: 16/250 | Batch: 1/527 | Loss: 5.127318859100342\n",
            "Epoch: 16/250 | Average Epoch Loss: 5.1418997938311755\n",
            "Epoch: 17/250 | Batch: 1/527 | Loss: 5.16706657409668\n",
            "Epoch: 17/250 | Average Epoch Loss: 5.082567558795271\n",
            "Epoch: 18/250 | Batch: 1/527 | Loss: 5.2795023918151855\n",
            "Epoch: 18/250 | Average Epoch Loss: 5.025694451703066\n",
            "Epoch: 19/250 | Batch: 1/527 | Loss: 4.890446186065674\n",
            "Epoch: 19/250 | Average Epoch Loss: 4.970334902660218\n",
            "Epoch: 20/250 | Batch: 1/527 | Loss: 4.892541885375977\n",
            "Epoch: 20/250 | Average Epoch Loss: 4.916401582605698\n",
            "Epoch: 21/250 | Batch: 1/527 | Loss: 4.948568344116211\n",
            "Epoch: 21/250 | Average Epoch Loss: 4.861930517815548\n",
            "Epoch: 22/250 | Batch: 1/527 | Loss: 4.665842056274414\n",
            "Epoch: 22/250 | Average Epoch Loss: 4.807552963778009\n",
            "Epoch: 23/250 | Batch: 1/527 | Loss: 4.574065685272217\n",
            "Epoch: 23/250 | Average Epoch Loss: 4.756796872140787\n",
            "Epoch: 24/250 | Batch: 1/527 | Loss: 4.9072184562683105\n",
            "Epoch: 24/250 | Average Epoch Loss: 4.70754499634484\n",
            "Epoch: 25/250 | Batch: 1/527 | Loss: 4.770630836486816\n",
            "Epoch: 25/250 | Average Epoch Loss: 4.658481757147714\n",
            "Epoch: 26/250 | Batch: 1/527 | Loss: 4.659687519073486\n",
            "Epoch: 26/250 | Average Epoch Loss: 4.606545559595387\n",
            "Epoch: 27/250 | Batch: 1/527 | Loss: 4.466081619262695\n",
            "Epoch: 27/250 | Average Epoch Loss: 4.558377109397295\n",
            "Epoch: 28/250 | Batch: 1/527 | Loss: 4.514509201049805\n",
            "Epoch: 28/250 | Average Epoch Loss: 4.508892449301153\n",
            "Epoch: 29/250 | Batch: 1/527 | Loss: 4.356353282928467\n",
            "Epoch: 29/250 | Average Epoch Loss: 4.459182896695508\n",
            "Epoch: 30/250 | Batch: 1/527 | Loss: 4.351256847381592\n",
            "Epoch: 30/250 | Average Epoch Loss: 4.417355309401336\n",
            "Epoch: 31/250 | Batch: 1/527 | Loss: 4.302955150604248\n",
            "Epoch: 31/250 | Average Epoch Loss: 4.366485467219489\n",
            "Epoch: 32/250 | Batch: 1/527 | Loss: 4.154510498046875\n",
            "Epoch: 32/250 | Average Epoch Loss: 4.3203345824464225\n",
            "Epoch: 33/250 | Batch: 1/527 | Loss: 4.324772357940674\n",
            "Epoch: 33/250 | Average Epoch Loss: 4.27700693964732\n",
            "Epoch: 34/250 | Batch: 1/527 | Loss: 4.413343906402588\n",
            "Epoch: 34/250 | Average Epoch Loss: 4.22573141626434\n",
            "Epoch: 35/250 | Batch: 1/527 | Loss: 4.204826354980469\n",
            "Epoch: 35/250 | Average Epoch Loss: 4.183847881132556\n",
            "Epoch: 36/250 | Batch: 1/527 | Loss: 4.167539596557617\n",
            "Epoch: 36/250 | Average Epoch Loss: 4.1392057362724755\n",
            "Epoch: 37/250 | Batch: 1/527 | Loss: 4.196341514587402\n",
            "Epoch: 37/250 | Average Epoch Loss: 4.09369262210332\n",
            "Epoch: 38/250 | Batch: 1/527 | Loss: 4.278139114379883\n",
            "Epoch: 38/250 | Average Epoch Loss: 4.055619822959972\n",
            "Epoch: 39/250 | Batch: 1/527 | Loss: 3.958359479904175\n",
            "Epoch: 39/250 | Average Epoch Loss: 4.005872104380569\n",
            "Epoch: 40/250 | Batch: 1/527 | Loss: 4.219668865203857\n",
            "Epoch: 40/250 | Average Epoch Loss: 3.959143286638061\n",
            "Epoch: 41/250 | Batch: 1/527 | Loss: 4.070947170257568\n",
            "Epoch: 41/250 | Average Epoch Loss: 3.9153262541913896\n",
            "Epoch: 42/250 | Batch: 1/527 | Loss: 4.00671911239624\n",
            "Epoch: 42/250 | Average Epoch Loss: 3.8756978588719524\n",
            "Epoch: 43/250 | Batch: 1/527 | Loss: 3.9175591468811035\n",
            "Epoch: 43/250 | Average Epoch Loss: 3.8276251859863977\n",
            "Epoch: 44/250 | Batch: 1/527 | Loss: 3.6585097312927246\n",
            "Epoch: 44/250 | Average Epoch Loss: 3.787390299947257\n",
            "Epoch: 45/250 | Batch: 1/527 | Loss: 3.7710466384887695\n",
            "Epoch: 45/250 | Average Epoch Loss: 3.7474456932784483\n",
            "Epoch: 46/250 | Batch: 1/527 | Loss: 3.896787405014038\n",
            "Epoch: 46/250 | Average Epoch Loss: 3.7054085478158334\n",
            "Epoch: 47/250 | Batch: 1/527 | Loss: 3.6250991821289062\n",
            "Epoch: 47/250 | Average Epoch Loss: 3.6673999936576136\n",
            "Epoch: 48/250 | Batch: 1/527 | Loss: 3.3403992652893066\n",
            "Epoch: 48/250 | Average Epoch Loss: 3.619777625154499\n",
            "Epoch: 49/250 | Batch: 1/527 | Loss: 3.903642177581787\n",
            "Epoch: 49/250 | Average Epoch Loss: 3.5849966405691185\n",
            "Epoch: 50/250 | Batch: 1/527 | Loss: 3.7025039196014404\n",
            "Epoch: 50/250 | Average Epoch Loss: 3.5453515428293363\n",
            "Epoch: 51/250 | Batch: 1/527 | Loss: 3.308983564376831\n",
            "Epoch: 51/250 | Average Epoch Loss: 3.4955799909877596\n",
            "Epoch: 52/250 | Batch: 1/527 | Loss: 3.4637458324432373\n",
            "Epoch: 52/250 | Average Epoch Loss: 3.4532642853101696\n",
            "Epoch: 53/250 | Batch: 1/527 | Loss: 3.5541419982910156\n",
            "Epoch: 53/250 | Average Epoch Loss: 3.413445975794059\n",
            "Epoch: 54/250 | Batch: 1/527 | Loss: 3.3290412425994873\n",
            "Epoch: 54/250 | Average Epoch Loss: 3.3771276790684044\n",
            "Epoch: 55/250 | Batch: 1/527 | Loss: 3.163633108139038\n",
            "Epoch: 55/250 | Average Epoch Loss: 3.336430787587754\n",
            "Epoch: 56/250 | Batch: 1/527 | Loss: 3.575550079345703\n",
            "Epoch: 56/250 | Average Epoch Loss: 3.2988828522430653\n",
            "Epoch: 57/250 | Batch: 1/527 | Loss: 3.063102960586548\n",
            "Epoch: 57/250 | Average Epoch Loss: 3.2488025126013858\n",
            "Epoch: 58/250 | Batch: 1/527 | Loss: 3.184539794921875\n",
            "Epoch: 58/250 | Average Epoch Loss: 3.22155214803721\n",
            "Epoch: 59/250 | Batch: 1/527 | Loss: 3.0361640453338623\n",
            "Epoch: 59/250 | Average Epoch Loss: 3.1769774855879724\n",
            "Epoch: 60/250 | Batch: 1/527 | Loss: 3.132366895675659\n",
            "Epoch: 60/250 | Average Epoch Loss: 3.134746289117513\n",
            "Epoch: 61/250 | Batch: 1/527 | Loss: 2.938124895095825\n",
            "Epoch: 61/250 | Average Epoch Loss: 3.094398931488819\n",
            "Epoch: 62/250 | Batch: 1/527 | Loss: 3.5001466274261475\n",
            "Epoch: 62/250 | Average Epoch Loss: 3.0719837762372997\n",
            "Epoch: 63/250 | Batch: 1/527 | Loss: 3.0837485790252686\n",
            "Epoch: 63/250 | Average Epoch Loss: 3.015252107460992\n",
            "Epoch: 64/250 | Batch: 1/527 | Loss: 2.6559481620788574\n",
            "Epoch: 64/250 | Average Epoch Loss: 2.977927485272373\n",
            "Epoch: 65/250 | Batch: 1/527 | Loss: 2.6648855209350586\n",
            "Epoch: 65/250 | Average Epoch Loss: 2.9398953254127864\n",
            "Epoch: 66/250 | Batch: 1/527 | Loss: 2.665010452270508\n",
            "Epoch: 66/250 | Average Epoch Loss: 2.914809804713703\n",
            "Epoch: 67/250 | Batch: 1/527 | Loss: 2.85294508934021\n",
            "Epoch: 67/250 | Average Epoch Loss: 2.861813341643824\n",
            "Epoch: 68/250 | Batch: 1/527 | Loss: 2.9646904468536377\n",
            "Epoch: 68/250 | Average Epoch Loss: 2.832965504286185\n",
            "Epoch: 69/250 | Batch: 1/527 | Loss: 2.6237096786499023\n",
            "Epoch: 69/250 | Average Epoch Loss: 2.791382196506015\n",
            "Epoch: 70/250 | Batch: 1/527 | Loss: 2.5916237831115723\n",
            "Epoch: 70/250 | Average Epoch Loss: 2.7546293667643074\n",
            "Epoch: 71/250 | Batch: 1/527 | Loss: 2.56839919090271\n",
            "Epoch: 71/250 | Average Epoch Loss: 2.723677995761386\n",
            "Epoch: 72/250 | Batch: 1/527 | Loss: 2.899322748184204\n",
            "Epoch: 72/250 | Average Epoch Loss: 2.695949124656547\n",
            "Epoch: 73/250 | Batch: 1/527 | Loss: 3.1313295364379883\n",
            "Epoch: 73/250 | Average Epoch Loss: 2.6472788490425705\n",
            "Epoch: 74/250 | Batch: 1/527 | Loss: 3.175539970397949\n",
            "Epoch: 74/250 | Average Epoch Loss: 2.6258347296850504\n",
            "Epoch: 75/250 | Batch: 1/527 | Loss: 2.334510564804077\n",
            "Epoch: 75/250 | Average Epoch Loss: 2.5665344739548397\n",
            "Epoch: 76/250 | Batch: 1/527 | Loss: 2.294220209121704\n",
            "Epoch: 76/250 | Average Epoch Loss: 2.5393604951746322\n",
            "Epoch: 77/250 | Batch: 1/527 | Loss: 2.2335493564605713\n",
            "Epoch: 77/250 | Average Epoch Loss: 2.5163180484265033\n",
            "Epoch: 78/250 | Batch: 1/527 | Loss: 2.8069796562194824\n",
            "Epoch: 78/250 | Average Epoch Loss: 2.4686319832557517\n",
            "Epoch: 79/250 | Batch: 1/527 | Loss: 2.230222463607788\n",
            "Epoch: 79/250 | Average Epoch Loss: 2.4335275424725644\n",
            "Epoch: 80/250 | Batch: 1/527 | Loss: 2.3899919986724854\n",
            "Epoch: 80/250 | Average Epoch Loss: 2.4065116732125036\n",
            "Epoch: 81/250 | Batch: 1/527 | Loss: 2.2970056533813477\n",
            "Epoch: 81/250 | Average Epoch Loss: 2.3797756218593533\n",
            "Epoch: 82/250 | Batch: 1/527 | Loss: 2.1442360877990723\n",
            "Epoch: 82/250 | Average Epoch Loss: 2.3367497138325586\n",
            "Epoch: 83/250 | Batch: 1/527 | Loss: 2.031507730484009\n",
            "Epoch: 83/250 | Average Epoch Loss: 2.3030463647118555\n",
            "Epoch: 84/250 | Batch: 1/527 | Loss: 1.945090413093567\n",
            "Epoch: 84/250 | Average Epoch Loss: 2.2782110851222694\n",
            "Epoch: 85/250 | Batch: 1/527 | Loss: 2.513375759124756\n",
            "Epoch: 85/250 | Average Epoch Loss: 2.2449060839086603\n",
            "Epoch: 86/250 | Batch: 1/527 | Loss: 2.2434914112091064\n",
            "Epoch: 86/250 | Average Epoch Loss: 2.2195994668486447\n",
            "Epoch: 87/250 | Batch: 1/527 | Loss: 2.23252010345459\n",
            "Epoch: 87/250 | Average Epoch Loss: 2.1995486738107237\n",
            "Epoch: 88/250 | Batch: 1/527 | Loss: 1.9315154552459717\n",
            "Epoch: 88/250 | Average Epoch Loss: 2.1513424046351743\n",
            "Epoch: 89/250 | Batch: 1/527 | Loss: 2.3664629459381104\n",
            "Epoch: 89/250 | Average Epoch Loss: 2.1162437578306252\n",
            "Epoch: 90/250 | Batch: 1/527 | Loss: 2.0638136863708496\n",
            "Epoch: 90/250 | Average Epoch Loss: 2.103699405460249\n",
            "Epoch: 91/250 | Batch: 1/527 | Loss: 1.8016985654830933\n",
            "Epoch: 91/250 | Average Epoch Loss: 2.0625418012915797\n",
            "Epoch: 92/250 | Batch: 1/527 | Loss: 1.759172797203064\n",
            "Epoch: 92/250 | Average Epoch Loss: 2.035720279366061\n",
            "Epoch: 93/250 | Batch: 1/527 | Loss: 1.9883205890655518\n",
            "Epoch: 93/250 | Average Epoch Loss: 2.0099037464927223\n",
            "Epoch: 94/250 | Batch: 1/527 | Loss: 2.2576494216918945\n",
            "Epoch: 94/250 | Average Epoch Loss: 1.9902807623430718\n",
            "Epoch: 95/250 | Batch: 1/527 | Loss: 1.6729629039764404\n",
            "Epoch: 95/250 | Average Epoch Loss: 1.9776855960516595\n",
            "Epoch: 96/250 | Batch: 1/527 | Loss: 1.6388188600540161\n",
            "Epoch: 96/250 | Average Epoch Loss: 1.9243472491088809\n",
            "Epoch: 97/250 | Batch: 1/527 | Loss: 1.9324179887771606\n",
            "Epoch: 97/250 | Average Epoch Loss: 1.909682949534856\n",
            "Epoch: 98/250 | Batch: 1/527 | Loss: 1.9145424365997314\n",
            "Epoch: 98/250 | Average Epoch Loss: 1.872721960920084\n",
            "Epoch: 99/250 | Batch: 1/527 | Loss: 1.5439693927764893\n",
            "Epoch: 99/250 | Average Epoch Loss: 1.8663561384863374\n",
            "Epoch: 100/250 | Batch: 1/527 | Loss: 1.5582081079483032\n",
            "Epoch: 100/250 | Average Epoch Loss: 1.8203649290146366\n",
            "Epoch: 101/250 | Batch: 1/527 | Loss: 2.1096208095550537\n",
            "Epoch: 101/250 | Average Epoch Loss: 1.8121897350904836\n",
            "Epoch: 102/250 | Batch: 1/527 | Loss: 2.418800115585327\n",
            "Epoch: 102/250 | Average Epoch Loss: 1.7827526263086801\n",
            "Epoch: 103/250 | Batch: 1/527 | Loss: 1.751236081123352\n",
            "Epoch: 103/250 | Average Epoch Loss: 1.7524698945557595\n",
            "Epoch: 104/250 | Batch: 1/527 | Loss: 1.4473004341125488\n",
            "Epoch: 104/250 | Average Epoch Loss: 1.7484695793781606\n",
            "Epoch: 105/250 | Batch: 1/527 | Loss: 2.0086634159088135\n",
            "Epoch: 105/250 | Average Epoch Loss: 1.7175751883798125\n",
            "Epoch: 106/250 | Batch: 1/527 | Loss: 1.4162298440933228\n",
            "Epoch: 106/250 | Average Epoch Loss: 1.6836010251597164\n",
            "Epoch: 107/250 | Batch: 1/527 | Loss: 1.6893948316574097\n",
            "Epoch: 107/250 | Average Epoch Loss: 1.6648695050425955\n",
            "Epoch: 108/250 | Batch: 1/527 | Loss: 1.7064571380615234\n",
            "Epoch: 108/250 | Average Epoch Loss: 1.666652720612412\n",
            "Epoch: 109/250 | Batch: 1/527 | Loss: 1.6234703063964844\n",
            "Epoch: 109/250 | Average Epoch Loss: 1.6480673918009257\n",
            "Epoch: 110/250 | Batch: 1/527 | Loss: 1.2921656370162964\n",
            "Epoch: 110/250 | Average Epoch Loss: 1.612928699723231\n",
            "Epoch: 111/250 | Batch: 1/527 | Loss: 1.2830853462219238\n",
            "Epoch: 111/250 | Average Epoch Loss: 1.6162094253290311\n",
            "Epoch: 112/250 | Batch: 1/527 | Loss: 1.3050612211227417\n",
            "Epoch: 112/250 | Average Epoch Loss: 1.5742416404456296\n",
            "Epoch: 113/250 | Batch: 1/527 | Loss: 1.2407069206237793\n",
            "Epoch: 113/250 | Average Epoch Loss: 1.5755844514329247\n",
            "Epoch: 114/250 | Batch: 1/527 | Loss: 1.2204844951629639\n",
            "Epoch: 114/250 | Average Epoch Loss: 1.5619370849128014\n",
            "Epoch: 115/250 | Batch: 1/527 | Loss: 1.2136330604553223\n",
            "Epoch: 115/250 | Average Epoch Loss: 1.562292788015145\n",
            "Epoch: 116/250 | Batch: 1/527 | Loss: 1.5385348796844482\n",
            "Epoch: 116/250 | Average Epoch Loss: 1.5352049588704697\n",
            "Epoch: 117/250 | Batch: 1/527 | Loss: 1.8633792400360107\n",
            "Epoch: 117/250 | Average Epoch Loss: 1.5117321118457947\n",
            "Epoch: 118/250 | Batch: 1/527 | Loss: 1.5295902490615845\n",
            "Epoch: 118/250 | Average Epoch Loss: 1.5175180408036233\n",
            "Epoch: 119/250 | Batch: 1/527 | Loss: 1.512764811515808\n",
            "Epoch: 119/250 | Average Epoch Loss: 1.4956352484520292\n",
            "Epoch: 120/250 | Batch: 1/527 | Loss: 1.7969226837158203\n",
            "Epoch: 120/250 | Average Epoch Loss: 1.4636634382396314\n",
            "Epoch: 121/250 | Batch: 1/527 | Loss: 1.475592017173767\n",
            "Epoch: 121/250 | Average Epoch Loss: 1.446062415555487\n",
            "Epoch: 122/250 | Batch: 1/527 | Loss: 1.799814224243164\n",
            "Epoch: 122/250 | Average Epoch Loss: 1.4554245426713628\n",
            "Epoch: 123/250 | Batch: 1/527 | Loss: 1.7852141857147217\n",
            "Epoch: 123/250 | Average Epoch Loss: 1.4322960654969912\n",
            "Epoch: 124/250 | Batch: 1/527 | Loss: 1.760205864906311\n",
            "Epoch: 124/250 | Average Epoch Loss: 1.4349311336846686\n",
            "Epoch: 125/250 | Batch: 1/527 | Loss: 1.0588105916976929\n",
            "Epoch: 125/250 | Average Epoch Loss: 1.4119812008099493\n",
            "Epoch: 126/250 | Batch: 1/527 | Loss: 1.4105167388916016\n",
            "Epoch: 126/250 | Average Epoch Loss: 1.4072270327772542\n",
            "Epoch: 127/250 | Batch: 1/527 | Loss: 1.082260251045227\n",
            "Epoch: 127/250 | Average Epoch Loss: 1.3899020589958784\n",
            "Epoch: 128/250 | Batch: 1/527 | Loss: 1.7620660066604614\n",
            "Epoch: 128/250 | Average Epoch Loss: 1.3713910855649771\n",
            "Epoch: 129/250 | Batch: 1/527 | Loss: 1.409907579421997\n",
            "Epoch: 129/250 | Average Epoch Loss: 1.375319505552187\n",
            "Epoch: 130/250 | Batch: 1/527 | Loss: 1.7286254167556763\n",
            "Epoch: 130/250 | Average Epoch Loss: 1.3784707432906134\n",
            "Epoch: 131/250 | Batch: 1/527 | Loss: 1.7157676219940186\n",
            "Epoch: 131/250 | Average Epoch Loss: 1.3720874647261747\n",
            "Epoch: 132/250 | Batch: 1/527 | Loss: 1.3408446311950684\n",
            "Epoch: 132/250 | Average Epoch Loss: 1.349802877803455\n",
            "Epoch: 133/250 | Batch: 1/527 | Loss: 1.6808419227600098\n",
            "Epoch: 133/250 | Average Epoch Loss: 1.356688240108273\n",
            "Epoch: 134/250 | Batch: 1/527 | Loss: 1.0232338905334473\n",
            "Epoch: 134/250 | Average Epoch Loss: 1.3487251287845778\n",
            "Epoch: 135/250 | Batch: 1/527 | Loss: 1.3345534801483154\n",
            "Epoch: 135/250 | Average Epoch Loss: 1.3350401406496255\n",
            "Epoch: 136/250 | Batch: 1/527 | Loss: 1.330228328704834\n",
            "Epoch: 136/250 | Average Epoch Loss: 1.3363064380479266\n",
            "Epoch: 137/250 | Batch: 1/527 | Loss: 0.9796798229217529\n",
            "Epoch: 137/250 | Average Epoch Loss: 1.309874316659779\n",
            "Epoch: 138/250 | Batch: 1/527 | Loss: 1.331743597984314\n",
            "Epoch: 138/250 | Average Epoch Loss: 1.3008422059849951\n",
            "Epoch: 139/250 | Batch: 1/527 | Loss: 1.3007937669754028\n",
            "Epoch: 139/250 | Average Epoch Loss: 1.2910648471264052\n",
            "Epoch: 140/250 | Batch: 1/527 | Loss: 1.3011056184768677\n",
            "Epoch: 140/250 | Average Epoch Loss: 1.2921417679008542\n",
            "Epoch: 141/250 | Batch: 1/527 | Loss: 0.9783787131309509\n",
            "Epoch: 141/250 | Average Epoch Loss: 1.2864346325510594\n",
            "Epoch: 142/250 | Batch: 1/527 | Loss: 1.6334322690963745\n",
            "Epoch: 142/250 | Average Epoch Loss: 1.2616977803621165\n",
            "Epoch: 143/250 | Batch: 1/527 | Loss: 1.262974739074707\n",
            "Epoch: 143/250 | Average Epoch Loss: 1.275142779499575\n",
            "Epoch: 144/250 | Batch: 1/527 | Loss: 0.9478518962860107\n",
            "Epoch: 144/250 | Average Epoch Loss: 1.2743074389517421\n",
            "Epoch: 145/250 | Batch: 1/527 | Loss: 1.9989094734191895\n",
            "Epoch: 145/250 | Average Epoch Loss: 1.2693844543914867\n",
            "Epoch: 146/250 | Batch: 1/527 | Loss: 1.9822673797607422\n",
            "Epoch: 146/250 | Average Epoch Loss: 1.2701171356088974\n",
            "Epoch: 147/250 | Batch: 1/527 | Loss: 0.8706172704696655\n",
            "Epoch: 147/250 | Average Epoch Loss: 1.2542880072313196\n",
            "Epoch: 148/250 | Batch: 1/527 | Loss: 1.2505446672439575\n",
            "Epoch: 148/250 | Average Epoch Loss: 1.244624364308207\n",
            "Epoch: 149/250 | Batch: 1/527 | Loss: 2.007240056991577\n",
            "Epoch: 149/250 | Average Epoch Loss: 1.24382348067394\n",
            "Epoch: 150/250 | Batch: 1/527 | Loss: 0.8531808257102966\n",
            "Epoch: 150/250 | Average Epoch Loss: 1.2330324534446961\n",
            "Epoch: 151/250 | Batch: 1/527 | Loss: 0.8958459496498108\n",
            "Epoch: 151/250 | Average Epoch Loss: 1.2463889433039208\n",
            "Epoch: 152/250 | Batch: 1/527 | Loss: 0.8870113492012024\n",
            "Epoch: 152/250 | Average Epoch Loss: 1.2501937668961411\n",
            "Epoch: 153/250 | Batch: 1/527 | Loss: 0.8809175491333008\n",
            "Epoch: 153/250 | Average Epoch Loss: 1.2277654902532387\n",
            "Epoch: 154/250 | Batch: 1/527 | Loss: 0.9002037048339844\n",
            "Epoch: 154/250 | Average Epoch Loss: 1.2279117012385852\n",
            "Epoch: 155/250 | Batch: 1/527 | Loss: 0.9131351709365845\n",
            "Epoch: 155/250 | Average Epoch Loss: 1.228905232061245\n",
            "Epoch: 156/250 | Batch: 1/527 | Loss: 0.8948624134063721\n",
            "Epoch: 156/250 | Average Epoch Loss: 1.2222389889849206\n",
            "Epoch: 157/250 | Batch: 1/527 | Loss: 0.8452603816986084\n",
            "Epoch: 157/250 | Average Epoch Loss: 1.2048213519226667\n",
            "Epoch: 158/250 | Batch: 1/527 | Loss: 0.8668482303619385\n",
            "Epoch: 158/250 | Average Epoch Loss: 1.2005889389727555\n",
            "Epoch: 159/250 | Batch: 1/527 | Loss: 1.2132192850112915\n",
            "Epoch: 159/250 | Average Epoch Loss: 1.2075243392072095\n",
            "Epoch: 160/250 | Batch: 1/527 | Loss: 1.94913911819458\n",
            "Epoch: 160/250 | Average Epoch Loss: 1.2169322217664411\n",
            "Epoch: 161/250 | Batch: 1/527 | Loss: 2.275266408920288\n",
            "Epoch: 161/250 | Average Epoch Loss: 1.1916415324925924\n",
            "Epoch: 162/250 | Batch: 1/527 | Loss: 1.1918765306472778\n",
            "Epoch: 162/250 | Average Epoch Loss: 1.1951054526913552\n",
            "Epoch: 163/250 | Batch: 1/527 | Loss: 1.887959361076355\n",
            "Epoch: 163/250 | Average Epoch Loss: 1.191802074266792\n",
            "Epoch: 164/250 | Batch: 1/527 | Loss: 1.207435965538025\n",
            "Epoch: 164/250 | Average Epoch Loss: 1.1968258137494834\n",
            "Epoch: 165/250 | Batch: 1/527 | Loss: 1.2067675590515137\n",
            "Epoch: 165/250 | Average Epoch Loss: 1.164512536444293\n",
            "Epoch: 166/250 | Batch: 1/527 | Loss: 1.1869738101959229\n",
            "Epoch: 166/250 | Average Epoch Loss: 1.1718920367492445\n",
            "Epoch: 167/250 | Batch: 1/527 | Loss: 0.8511831164360046\n",
            "Epoch: 167/250 | Average Epoch Loss: 1.172520838833398\n",
            "Epoch: 168/250 | Batch: 1/527 | Loss: 1.519344449043274\n",
            "Epoch: 168/250 | Average Epoch Loss: 1.186457911750171\n",
            "Epoch: 169/250 | Batch: 1/527 | Loss: 1.2120919227600098\n",
            "Epoch: 169/250 | Average Epoch Loss: 1.1689588009067007\n",
            "Epoch: 170/250 | Batch: 1/527 | Loss: 0.8193368911743164\n",
            "Epoch: 170/250 | Average Epoch Loss: 1.1875410135375932\n",
            "Epoch: 171/250 | Batch: 1/527 | Loss: 1.1730992794036865\n",
            "Epoch: 171/250 | Average Epoch Loss: 1.1625975770787451\n",
            "Epoch: 172/250 | Batch: 1/527 | Loss: 1.1648123264312744\n",
            "Epoch: 172/250 | Average Epoch Loss: 1.1532495035392283\n",
            "Epoch: 173/250 | Batch: 1/527 | Loss: 1.1463549137115479\n",
            "Epoch: 173/250 | Average Epoch Loss: 1.1343370150117313\n",
            "Epoch: 174/250 | Batch: 1/527 | Loss: 1.164145827293396\n",
            "Epoch: 174/250 | Average Epoch Loss: 1.1684128049428821\n",
            "Epoch: 175/250 | Batch: 1/527 | Loss: 1.1857199668884277\n",
            "Epoch: 175/250 | Average Epoch Loss: 1.139666026185993\n",
            "Epoch: 176/250 | Batch: 1/527 | Loss: 1.5617077350616455\n",
            "Epoch: 176/250 | Average Epoch Loss: 1.1467026570490235\n",
            "Epoch: 177/250 | Batch: 1/527 | Loss: 1.1675724983215332\n",
            "Epoch: 177/250 | Average Epoch Loss: 1.1504627888071468\n",
            "Epoch: 178/250 | Batch: 1/527 | Loss: 1.1671037673950195\n",
            "Epoch: 178/250 | Average Epoch Loss: 1.135556310358717\n",
            "Epoch: 179/250 | Batch: 1/527 | Loss: 1.1595790386199951\n",
            "Epoch: 179/250 | Average Epoch Loss: 1.1432604756934366\n",
            "Epoch: 180/250 | Batch: 1/527 | Loss: 0.7834712862968445\n",
            "Epoch: 180/250 | Average Epoch Loss: 1.1368372967392488\n",
            "Epoch: 181/250 | Batch: 1/527 | Loss: 0.7896307110786438\n",
            "Epoch: 181/250 | Average Epoch Loss: 1.1316794229413345\n",
            "Epoch: 182/250 | Batch: 1/527 | Loss: 1.169567584991455\n",
            "Epoch: 182/250 | Average Epoch Loss: 1.1153739311889634\n",
            "Epoch: 183/250 | Batch: 1/527 | Loss: 0.8049737811088562\n",
            "Epoch: 183/250 | Average Epoch Loss: 1.1366420016116843\n",
            "Epoch: 184/250 | Batch: 1/527 | Loss: 0.7539785504341125\n",
            "Epoch: 184/250 | Average Epoch Loss: 1.1259970400319832\n",
            "Epoch: 185/250 | Batch: 1/527 | Loss: 0.7856953144073486\n",
            "Epoch: 185/250 | Average Epoch Loss: 1.1368931841805041\n",
            "Epoch: 186/250 | Batch: 1/527 | Loss: 1.8679986000061035\n",
            "Epoch: 186/250 | Average Epoch Loss: 1.1117123087171812\n",
            "Epoch: 187/250 | Batch: 1/527 | Loss: 0.7695205807685852\n",
            "Epoch: 187/250 | Average Epoch Loss: 1.115001228774069\n",
            "Epoch: 188/250 | Batch: 1/527 | Loss: 1.176688313484192\n",
            "Epoch: 188/250 | Average Epoch Loss: 1.1234017525723583\n",
            "Epoch: 189/250 | Batch: 1/527 | Loss: 1.1498336791992188\n",
            "Epoch: 189/250 | Average Epoch Loss: 1.145564509297684\n",
            "Epoch: 190/250 | Batch: 1/527 | Loss: 1.149322271347046\n",
            "Epoch: 190/250 | Average Epoch Loss: 1.1193829162975868\n",
            "Epoch: 191/250 | Batch: 1/527 | Loss: 0.7659814953804016\n",
            "Epoch: 191/250 | Average Epoch Loss: 1.1018686145261298\n",
            "Epoch: 192/250 | Batch: 1/527 | Loss: 1.1377283334732056\n",
            "Epoch: 192/250 | Average Epoch Loss: 1.1278542661350186\n",
            "Epoch: 193/250 | Batch: 1/527 | Loss: 0.7588627338409424\n",
            "Epoch: 193/250 | Average Epoch Loss: 1.1072168579816366\n",
            "Epoch: 194/250 | Batch: 1/527 | Loss: 1.1249475479125977\n",
            "Epoch: 194/250 | Average Epoch Loss: 1.1191221094448154\n",
            "Epoch: 195/250 | Batch: 1/527 | Loss: 0.7606515288352966\n",
            "Epoch: 195/250 | Average Epoch Loss: 1.1002688800361171\n",
            "Epoch: 196/250 | Batch: 1/527 | Loss: 1.1015554666519165\n",
            "Epoch: 196/250 | Average Epoch Loss: 1.0960126032865478\n",
            "Epoch: 197/250 | Batch: 1/527 | Loss: 1.8499871492385864\n",
            "Epoch: 197/250 | Average Epoch Loss: 1.1067235411005183\n",
            "Epoch: 198/250 | Batch: 1/527 | Loss: 0.7745470404624939\n",
            "Epoch: 198/250 | Average Epoch Loss: 1.1047450996214343\n",
            "Epoch: 199/250 | Batch: 1/527 | Loss: 1.1260215044021606\n",
            "Epoch: 199/250 | Average Epoch Loss: 1.1034920839928586\n",
            "Epoch: 200/250 | Batch: 1/527 | Loss: 1.4888916015625\n",
            "Epoch: 200/250 | Average Epoch Loss: 1.0959205988236125\n",
            "Epoch: 201/250 | Batch: 1/527 | Loss: 1.1017457246780396\n",
            "Epoch: 201/250 | Average Epoch Loss: 1.108915893357438\n",
            "Epoch: 202/250 | Batch: 1/527 | Loss: 1.4775841236114502\n",
            "Epoch: 202/250 | Average Epoch Loss: 1.0982176310875837\n",
            "Epoch: 203/250 | Batch: 1/527 | Loss: 0.7289850115776062\n",
            "Epoch: 203/250 | Average Epoch Loss: 1.1057351967867683\n",
            "Epoch: 204/250 | Batch: 1/527 | Loss: 1.820261836051941\n",
            "Epoch: 204/250 | Average Epoch Loss: 1.0904102999526137\n",
            "Epoch: 205/250 | Batch: 1/527 | Loss: 1.1480287313461304\n",
            "Epoch: 205/250 | Average Epoch Loss: 1.0936932670097423\n",
            "Epoch: 206/250 | Batch: 1/527 | Loss: 1.1088128089904785\n",
            "Epoch: 206/250 | Average Epoch Loss: 1.0862301973735584\n",
            "Epoch: 207/250 | Batch: 1/527 | Loss: 1.4467089176177979\n",
            "Epoch: 207/250 | Average Epoch Loss: 1.0971147982400102\n",
            "Epoch: 208/250 | Batch: 1/527 | Loss: 1.4886970520019531\n",
            "Epoch: 208/250 | Average Epoch Loss: 1.099043858119161\n",
            "Epoch: 209/250 | Batch: 1/527 | Loss: 0.7587437033653259\n",
            "Epoch: 209/250 | Average Epoch Loss: 1.0843421829267064\n",
            "Epoch: 210/250 | Batch: 1/527 | Loss: 0.7147290706634521\n",
            "Epoch: 210/250 | Average Epoch Loss: 1.0840574465621355\n",
            "Epoch: 211/250 | Batch: 1/527 | Loss: 1.4500302076339722\n",
            "Epoch: 211/250 | Average Epoch Loss: 1.077307237167286\n",
            "Epoch: 212/250 | Batch: 1/527 | Loss: 0.7349380254745483\n",
            "Epoch: 212/250 | Average Epoch Loss: 1.0734317523252352\n",
            "Epoch: 213/250 | Batch: 1/527 | Loss: 0.738894522190094\n",
            "Epoch: 213/250 | Average Epoch Loss: 1.0796800901586687\n",
            "Epoch: 214/250 | Batch: 1/527 | Loss: 1.4624367952346802\n",
            "Epoch: 214/250 | Average Epoch Loss: 1.061750947750497\n",
            "Epoch: 215/250 | Batch: 1/527 | Loss: 0.7327808737754822\n",
            "Epoch: 215/250 | Average Epoch Loss: 1.0701340507058537\n",
            "Epoch: 216/250 | Batch: 1/527 | Loss: 1.4426956176757812\n",
            "Epoch: 216/250 | Average Epoch Loss: 1.0542817346511348\n",
            "Epoch: 217/250 | Batch: 1/527 | Loss: 1.069854974746704\n",
            "Epoch: 217/250 | Average Epoch Loss: 1.0834716128669608\n",
            "Epoch: 218/250 | Batch: 1/527 | Loss: 0.7041046619415283\n",
            "Epoch: 218/250 | Average Epoch Loss: 1.0703651137098642\n",
            "Epoch: 219/250 | Batch: 1/527 | Loss: 0.7252445220947266\n",
            "Epoch: 219/250 | Average Epoch Loss: 1.062936808058614\n",
            "Epoch: 220/250 | Batch: 1/527 | Loss: 0.7162070870399475\n",
            "Epoch: 220/250 | Average Epoch Loss: 1.0692867431966357\n",
            "Epoch: 221/250 | Batch: 1/527 | Loss: 1.4460759162902832\n",
            "Epoch: 221/250 | Average Epoch Loss: 1.0738843258689432\n",
            "Epoch: 222/250 | Batch: 1/527 | Loss: 1.4372614622116089\n",
            "Epoch: 222/250 | Average Epoch Loss: 1.062709361032019\n",
            "Epoch: 223/250 | Batch: 1/527 | Loss: 0.7146587371826172\n",
            "Epoch: 223/250 | Average Epoch Loss: 1.0636876459592208\n",
            "Epoch: 224/250 | Batch: 1/527 | Loss: 0.7391365170478821\n",
            "Epoch: 224/250 | Average Epoch Loss: 1.0620651695262322\n",
            "Epoch: 225/250 | Batch: 1/527 | Loss: 1.4312784671783447\n",
            "Epoch: 225/250 | Average Epoch Loss: 1.0551548535728816\n",
            "Epoch: 226/250 | Batch: 1/527 | Loss: 1.4616135358810425\n",
            "Epoch: 226/250 | Average Epoch Loss: 1.0734293728897195\n",
            "Epoch: 227/250 | Batch: 1/527 | Loss: 1.769824504852295\n",
            "Epoch: 227/250 | Average Epoch Loss: 1.0543450358018025\n",
            "Epoch: 228/250 | Batch: 1/527 | Loss: 1.4276249408721924\n",
            "Epoch: 228/250 | Average Epoch Loss: 1.0387546590881058\n",
            "Epoch: 229/250 | Batch: 1/527 | Loss: 1.437921404838562\n",
            "Epoch: 229/250 | Average Epoch Loss: 1.0188091289838759\n",
            "Epoch: 230/250 | Batch: 1/527 | Loss: 1.0798336267471313\n",
            "Epoch: 230/250 | Average Epoch Loss: 1.0687335536194933\n",
            "Epoch: 231/250 | Batch: 1/527 | Loss: 1.436307430267334\n",
            "Epoch: 231/250 | Average Epoch Loss: 1.0541346575322594\n",
            "Epoch: 232/250 | Batch: 1/527 | Loss: 0.718143880367279\n",
            "Epoch: 232/250 | Average Epoch Loss: 1.0434530580971226\n",
            "Epoch: 233/250 | Batch: 1/527 | Loss: 1.0533024072647095\n",
            "Epoch: 233/250 | Average Epoch Loss: 1.0643135595366895\n",
            "Epoch: 234/250 | Batch: 1/527 | Loss: 0.6955786347389221\n",
            "Epoch: 234/250 | Average Epoch Loss: 1.043358046483722\n",
            "Epoch: 235/250 | Batch: 1/527 | Loss: 1.7779806852340698\n",
            "Epoch: 235/250 | Average Epoch Loss: 1.0559023881094278\n",
            "Epoch: 236/250 | Batch: 1/527 | Loss: 0.6803584694862366\n",
            "Epoch: 236/250 | Average Epoch Loss: 1.031673350868008\n",
            "Epoch: 237/250 | Batch: 1/527 | Loss: 1.0387028455734253\n",
            "Epoch: 237/250 | Average Epoch Loss: 1.0390189025388497\n",
            "Epoch: 238/250 | Batch: 1/527 | Loss: 0.6990100741386414\n",
            "Epoch: 238/250 | Average Epoch Loss: 1.032276674172005\n",
            "Epoch: 239/250 | Batch: 1/527 | Loss: 0.7071042656898499\n",
            "Epoch: 239/250 | Average Epoch Loss: 1.0242518904765598\n",
            "Epoch: 240/250 | Batch: 1/527 | Loss: 1.0668606758117676\n",
            "Epoch: 240/250 | Average Epoch Loss: 1.034695651884097\n",
            "Epoch: 241/250 | Batch: 1/527 | Loss: 1.4100520610809326\n",
            "Epoch: 241/250 | Average Epoch Loss: 1.0518884404334443\n",
            "Epoch: 242/250 | Batch: 1/527 | Loss: 1.3949158191680908\n",
            "Epoch: 242/250 | Average Epoch Loss: 1.0425534891901251\n",
            "Epoch: 243/250 | Batch: 1/527 | Loss: 0.7017945647239685\n",
            "Epoch: 243/250 | Average Epoch Loss: 1.0313609206246697\n",
            "Epoch: 244/250 | Batch: 1/527 | Loss: 1.038441777229309\n",
            "Epoch: 244/250 | Average Epoch Loss: 1.025783276987709\n",
            "Epoch: 245/250 | Batch: 1/527 | Loss: 0.6920114755630493\n",
            "Epoch: 245/250 | Average Epoch Loss: 1.0293525646262196\n",
            "Epoch: 246/250 | Batch: 1/527 | Loss: 0.7090440392494202\n",
            "Epoch: 246/250 | Average Epoch Loss: 1.0303571949195138\n",
            "Epoch: 247/250 | Batch: 1/527 | Loss: 0.7106754183769226\n",
            "Epoch: 247/250 | Average Epoch Loss: 1.0404103105389415\n",
            "Epoch: 248/250 | Batch: 1/527 | Loss: 1.02681303024292\n",
            "Epoch: 248/250 | Average Epoch Loss: 1.016006455249533\n",
            "Epoch: 249/250 | Batch: 1/527 | Loss: 1.0362094640731812\n",
            "Epoch: 249/250 | Average Epoch Loss: 1.0354545152617134\n",
            "Epoch: 250/250 | Batch: 1/527 | Loss: 0.684084951877594\n",
            "Epoch: 250/250 | Average Epoch Loss: 1.0297513330457786\n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "model_stats = model.train_model(loader=train_loader, optimizer=optimizer, criterion=criterion, epochs=250, print_every=1000)\n",
        "\n",
        "# save model dict\n",
        "model_appendix = str(time.time())\n",
        "torch.save(model_stats, \"drive/My Drive/model_stats_{}.pt\".format(model_appendix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Bncn1u9GDP"
      },
      "source": [
        "# Evaluierung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HcRCpW29GDQ"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, model, dataloader, device):\n",
        "        # initiate variables \n",
        "        self.model = model\n",
        "        self.dataloader = dataloader\n",
        "        self.device = device\n",
        "        # self.model.eval()\n",
        "        assert self.dataloader.batch_size == 1, \"Batch size must be 1 for evaluation.\"\n",
        "    \n",
        "    def evaluate(self):\n",
        "        scores = []\n",
        "\n",
        "        for i, (images, captions, lengths, vectorized_captions) in enumerate(self.dataloader):\n",
        "            # move to device\n",
        "            images = images.to(self.device)\n",
        "            captions = captions.to(self.device)\n",
        "            vectorized_captions = vectorized_captions.to(self.device)\n",
        "            \n",
        "            # forward pass\n",
        "            output = self.model.forward(images)[0]\n",
        "            candidate = self.output_to_sentence(output)\n",
        "            reference = self.output_to_sentence(self.model.embedding.index_to_caption(vectorized_captions.permute(1,0))[0])\n",
        "\n",
        "            # calculate bleu score\n",
        "            bleu_score = self.bleu_score(reference, candidate)\n",
        "            scores.append(bleu_score)\n",
        "\n",
        "        return np.mean(scores), scores\n",
        "\n",
        "    @staticmethod\n",
        "    def output_to_sentence(output:list):\n",
        "        '''\n",
        "        Removes Tokens from model output.\n",
        "        '''\n",
        "        output = [token for token in output if token not in [\"<SOS>\", \"<EOS>\", \"<PAD>\"]]\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def bleu_score(reference, candidate):\n",
        "        '''\n",
        "        Calculates the BLEU score for a single reference and candidate. Uses the SmoothingFunction for smoothing when no overlap between certain n-grams is found. \n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "        reference: list of strings - The reference sentence.\n",
        "        candidate: list of strings - The candidate sentence.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        bleu_score: float - The BLEU score.\n",
        "        '''\n",
        "        # calculate the BLEU score\n",
        "        return nltk.translate.bleu_score.sentence_bleu(reference, candidate, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NttpE3AhHt1M"
      },
      "outputs": [],
      "source": [
        "path = 'drive/MyDrive/model_stats_1667610807.4613428.pt'\n",
        "\n",
        "model_stats = torch.load(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U331derr9GDR",
        "outputId": "4f1e2b58-674b-4c0d-fc0c-afc9ccaf87c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train BLEU: 0.010615250434601056\n",
            "Test BLEU: 0.01060335546430951\n"
          ]
        }
      ],
      "source": [
        "train_dataset = FlickrDataset(captions=training_data, embedding=embedding)\n",
        "test_dataset = FlickrDataset(captions=test_data, embedding=embedding)\n",
        "\n",
        "# create dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, drop_last=True)\n",
        "\n",
        "# load captioning model\n",
        "model = load_captioning_model(model_stats)\n",
        "\n",
        "# calculate bleu scores\n",
        "train_evaluator = Evaluator(model, train_loader, device)\n",
        "test_evaluator = Evaluator(model, test_loader, device)\n",
        "\n",
        "train_bleu, train_scores = train_evaluator.evaluate()\n",
        "test_bleu, test_scores = test_evaluator.evaluate()\n",
        "\n",
        "print(f\"Train BLEU: {train_bleu}\")\n",
        "print(f\"Test BLEU: {test_bleu}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "F7ggCETV9GDS"
      },
      "outputs": [],
      "source": [
        "# export bleu scores\n",
        "with open(\"drive/My Drive/train_scores_{}.pkl\".format(\"_with_normalisation\"), \"wb\") as f:\n",
        "    pickle.dump(train_scores, f)\n",
        "\n",
        "with open(\"drive/My Drive/test_scores_{}.pkl\".format(\"_with_normalisation\"), \"wb\") as f:\n",
        "    pickle.dump(test_scores, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6D55YtF218F"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.9.15 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "8c062c6b57d91616a29c64ccda85a992f94b9c302ff6b9e6bdfcfbfa090602a1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}