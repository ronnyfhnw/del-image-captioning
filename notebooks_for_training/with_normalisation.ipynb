{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "23FSvPla9GDC"
      },
      "outputs": [],
      "source": [
        "from img_cap_lib import *\n",
        "# imports\n",
        "import torch\n",
        "import torchvision\n",
        "import torchtext\n",
        "from torchtext.vocab import vocab, GloVe, Vectors\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import os\n",
        "from PIL import Image\n",
        "import string\n",
        "from collections import OrderedDict, Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import os\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EFpN7FJu9Kd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b30e3d-8b3e-416b-f950-b2342c37c34d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImcPlOrB9GDG"
      },
      "source": [
        "# Daten herunterladen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Or3FdVAM9GDH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c82186-a16b-414a-ba9c-93b1640dd650"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data already exists at flickr8k\n"
          ]
        }
      ],
      "source": [
        "data_download(\"flickr8k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZovGtMKV9GDK"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FdgY3n_J9GDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530056bc-8b1e-42e2-989e-d0b3ff50a876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape captions: (40460, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[name] = value\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape captions after filtering: (40119, 3)\n",
            "Removed Captions:  341 , in Percent:  0.84\n",
            "transformed_images folder already exists. No preprocessing necessary.\n"
          ]
        }
      ],
      "source": [
        "# caption preprocessing\n",
        "embedding_dim = 300\n",
        "min_frequency = 1\n",
        "\n",
        "captions = pd.read_csv(\"flickr8k/captions.txt\")\n",
        "caption_preprocessor = CaptionPreprocessor(captions=captions, embedding_dim=embedding_dim, min_frequency=min_frequency)\n",
        "caption_preprocessor.preprocess()\n",
        "\n",
        "# image preprocessing\n",
        "img_preprocessor = ImagePreprocessor(normalize=True, image_folder_path=\"flickr8k\")\n",
        "img_preprocessor.preprocess_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nl3NT_K9GDL"
      },
      "source": [
        "# Data Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aQ07VI9R9GDM"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "training_data, test_data = train_test_split(caption_preprocessor.captions, test_size=0.15, random_state=42)\n",
        "\n",
        "embedding = Embedding(embedding_matrix=caption_preprocessor.embedding, vocabulary=caption_preprocessor.vocabulary)\n",
        "\n",
        "# create dataset\n",
        "train_dataset = FlickrDataset(captions=training_data, embedding=embedding)\n",
        "test_dataset = FlickrDataset(captions=test_data, embedding=embedding)\n",
        "\n",
        "# create dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-254t2-t9GDN"
      },
      "source": [
        "# Modell erstellen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s8MBnXU09GDN"
      },
      "outputs": [],
      "source": [
        "encoder = EncoderCNN(net=torchvision.models.resnext50_32x4d, pretrained_weights=torchvision.models.ResNeXt50_32X4D_Weights.IMAGENET1K_V2, output_size=300)\n",
        "decoder = DecoderRNN(input_size=300, hidden_size=caption_preprocessor.embedding_dim, num_layers=1, dropout=0.0, len_vocab=8752)\n",
        "\n",
        "model = ImageCaptioning(encoder=encoder, decoder=decoder, embedding=embedding, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.embedding.caption_to_embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GYGQhq9X9KL",
        "outputId": "2501df27-c4bb-42ab-90f2-082ba7794ded"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Embedding.caption_to_embedding of <img_cap_lib.Embedding object at 0x7fb39f014e90>>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "02luogN-9GDO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "outputId": "eda00eb2-46a5-4d7a-d14c-1efaabd29761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([23, 64, 300])\n",
            "torch.Size([19, 64, 8752])\n",
            "torch.Size([22, 64, 8752])\n",
            "torch.Size([64, 8752, 22]) torch.Size([64, 22])\n",
            "Epoch: 1/250 | Batch: 1/532 | Loss: 8.932557106018066\n",
            "torch.Size([23, 64, 300])\n",
            "torch.Size([21, 64, 8752])\n",
            "torch.Size([22, 64, 8752])\n",
            "torch.Size([64, 8752, 22]) torch.Size([64, 22])\n",
            "torch.Size([23, 64, 300])\n",
            "torch.Size([20, 64, 8752])\n",
            "torch.Size([22, 64, 8752])\n",
            "torch.Size([64, 8752, 22]) torch.Size([64, 22])\n",
            "torch.Size([23, 64, 300])\n",
            "torch.Size([22, 64, 8752])\n",
            "torch.Size([22, 64, 8752])\n",
            "torch.Size([64, 8752, 22]) torch.Size([64, 22])\n",
            "torch.Size([23, 64, 300])\n",
            "torch.Size([23, 64, 8752])\n",
            "torch.Size([44, 64, 8752])\n",
            "torch.Size([64, 8752, 44]) torch.Size([64, 22])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0171b1f80ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# save model dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/img_cap_lib.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, loader, optimizer, criterion, epochs, print_every)\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized_captions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorized_captions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1165\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3013\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3014\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected target size [64, 44], got [64, 22]"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "model_stats = model.train_model(loader=train_loader, optimizer=optimizer, criterion=criterion, epochs=250, print_every=1000)\n",
        "\n",
        "# save model dict\n",
        "model_appendix = str(time.time())\n",
        "torch.save(model_stats, \"drive/My Drive/model_stats_{}.pt\".format(model_appendix))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5Bncn1u9GDP"
      },
      "source": [
        "# Evaluierung"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HcRCpW29GDQ"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "    def __init__(self, model, dataloader, device):\n",
        "        # initiate variables \n",
        "        self.model = model\n",
        "        self.dataloader = dataloader\n",
        "        self.device = device\n",
        "        # self.model.eval()\n",
        "        assert self.dataloader.batch_size == 1, \"Batch size must be 1 for evaluation.\"\n",
        "    \n",
        "    def evaluate(self):\n",
        "        scores = []\n",
        "\n",
        "        for i, (images, captions, lengths, vectorized_captions) in enumerate(self.dataloader):\n",
        "            # move to device\n",
        "            images = images.to(self.device)\n",
        "            captions = captions.to(self.device)\n",
        "            vectorized_captions = vectorized_captions.to(self.device)\n",
        "            \n",
        "            # forward pass\n",
        "            output = self.model.forward(images)[0]\n",
        "            candidate = self.output_to_sentence(output)\n",
        "            reference = self.output_to_sentence(self.model.embedding.index_to_caption(vectorized_captions.permute(1,0))[0])\n",
        "\n",
        "            # calculate bleu score\n",
        "            bleu_score = self.bleu_score(reference, candidate)\n",
        "            scores.append(bleu_score)\n",
        "\n",
        "        return np.mean(scores), scores\n",
        "\n",
        "    @staticmethod\n",
        "    def output_to_sentence(output:list):\n",
        "        '''\n",
        "        Removes Tokens from model output.\n",
        "        '''\n",
        "        output = [token for token in output if token not in [\"<SOS>\", \"<EOS>\", \"<PAD>\"]]\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def bleu_score(reference, candidate):\n",
        "        '''\n",
        "        Calculates the BLEU score for a single reference and candidate. Uses the SmoothingFunction for smoothing when no overlap between certain n-grams is found. \n",
        "\n",
        "        Params:\n",
        "        -------\n",
        "        reference: list of strings - The reference sentence.\n",
        "        candidate: list of strings - The candidate sentence.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        bleu_score: float - The BLEU score.\n",
        "        '''\n",
        "        # calculate the BLEU score\n",
        "        return nltk.translate.bleu_score.sentence_bleu(reference, candidate, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U331derr9GDR"
      },
      "outputs": [],
      "source": [
        "# calculate bleu scores\n",
        "train_evaluator = Evaluator(model, train_loader, device)\n",
        "test_evaluator = Evaluator(model, test_loader, device)\n",
        "\n",
        "train_bleu, train_scores = train_evaluator.evaluate()\n",
        "test_bleu, test_scores = test_evaluator.evaluate()\n",
        "\n",
        "print(f\"Train BLEU: {train_bleu}\")\n",
        "print(f\"Test BLEU: {test_bleu}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7ggCETV9GDS"
      },
      "outputs": [],
      "source": [
        "# export bleu scores\n",
        "with open(\"drive/My Drive/train_scores_{}.pkl\".format(model_appendix), \"wb\") as f:\n",
        "    pickle.dump(train_scores, f)\n",
        "\n",
        "with open(\"drive/My Drive/test_scores_{}.pkl\".format(model_appendix), \"wb\") as f:\n",
        "    pickle.dump(test_scores, f)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.15 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8c062c6b57d91616a29c64ccda85a992f94b9c302ff6b9e6bdfcfbfa090602a1"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}