{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from img_cap_lib import *\n",
    "# imports\n",
    "import torch\n",
    "import torchvision\n",
    "import torchtext\n",
    "from torchtext.vocab import vocab, GloVe, Vectors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import os\n",
    "from PIL import Image\n",
    "import string\n",
    "from collections import OrderedDict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daten herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists at flickr8k\n"
     ]
    }
   ],
   "source": [
    "data_download(\"flickr8k_copy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modell laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model_stats = torch.load(\"models/model_stats_1667495086.9660888.pt\", map_location=torch.device('cpu'))\n",
    "model = load_captioning_model(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape captions: (40460, 2)\n",
      "Shape captions after filtering: (40119, 3)\n",
      "Removed Captions:  341 , in Percent:  0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ronnyschneeberger/Documents/FHNW/HS22/del-image-captioning/img_cap_lib.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.captions.caption = self.captions.caption.apply(lambda x: x.strip(\".\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed_images folder already exists. No preprocessing necessary.\n"
     ]
    }
   ],
   "source": [
    "# caption preprocessing\n",
    "embedding_dim = 300\n",
    "min_frequency = 1\n",
    "\n",
    "captions = pd.read_csv(\"flickr8k_copy/captions.txt\")\n",
    "caption_preprocessor = CaptionPreprocessor(embedding=model_stats['embedding'].embedding_matrix, vocabulary=model_stats['embedding'].vocabulary ,captions=captions, embedding_dim=embedding_dim, min_frequency=min_frequency)\n",
    "caption_preprocessor.preprocess()\n",
    "\n",
    "# image preprocessing\n",
    "img_preprocessor = ImagePreprocessor(normalize=False, image_folder_path=\"flickr8k_copy\")\n",
    "img_preprocessor.preprocess_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datensplit und DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create split\n",
    "training_data, test_data = train_test_split(caption_preprocessor.captions, test_size=0.15, random_state=42)\n",
    "\n",
    "# create datasets\n",
    "train_dataset = FlickrDataset(captions=training_data, embedding=model.embedding)\n",
    "test_dataset = FlickrDataset(captions=test_data, embedding=model.embedding)\n",
    "\n",
    "# create dataloaders\n",
    "batch_size = 1\n",
    "train_loader = FlickrLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = FlickrLoader(test_dataset, batch_size=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, model, dataloader, device):\n",
    "        # initiate variables \n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        assert self.dataloader.batch_size == 1, \"Batch size must be 1 for evaluation.\"\n",
    "    \n",
    "    def evaluate(self):\n",
    "        scores = []\n",
    "\n",
    "        for i, (images, captions, lengths, vectorized_captions) in enumerate(self.dataloader):\n",
    "            # move to device\n",
    "            images = images.to(self.device)\n",
    "            captions = captions.to(self.device)\n",
    "            vectorized_captions = vectorized_captions.to(self.device)\n",
    "            \n",
    "            # forward pass\n",
    "            output = self.model.forward(images)[0]\n",
    "            candidate = self.output_to_sentence(output)\n",
    "            reference = self.output_to_sentence(self.model.embedding.index_to_caption(vectorized_captions).permute(1,0)[0])\n",
    "\n",
    "            # calculate bleu score\n",
    "            bleu_score = nltk.translate.bleu_score.sentence_bleu(reference, candidate)\n",
    "            scores.append(bleu_score)\n",
    "\n",
    "        return np.mean(scores), scores\n",
    "\n",
    "    @staticmethod\n",
    "    def output_to_sentence(output:list):\n",
    "        '''\n",
    "        Removes Tokens from model output.\n",
    "        '''\n",
    "        output = [token for token in output if token not in [\"<SOS>\", \"<EOS>\", \"<PAD>\"]]\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def bleu_score(reference, candidate):\n",
    "        '''\n",
    "        Calculates the BLEU score for a single reference and candidate. Uses the SmoothingFunction for smoothing when no overlap between certain n-grams is found. \n",
    "\n",
    "        Params:\n",
    "        -------\n",
    "        reference: list of strings - The reference sentence.\n",
    "        candidate: list of strings - The candidate sentence.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        bleu_score: float - The BLEU score.\n",
    "        '''\n",
    "        # calculate the BLEU score\n",
    "        return nltk.translate.bleu_score.sentence_bleu(reference, candidate, smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method1)\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c062c6b57d91616a29c64ccda85a992f94b9c302ff6b9e6bdfcfbfa090602a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
